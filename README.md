# MY OWN
this is a personal project where i develop basic concepts of machine learning and deep learning from scratch. 

the main purpose is to understand the core algorithms that i use everyday.

the path i'll follow is:
- learn the theory
- write a summary while i'm learning with my own words
- code the algorithm from scratch


## tokenizer:
is a way to chunk the text from a given sentence to encode into a numerical representation. this is done to feed the llm with a numerical representation of text, aka something that the llm can understand.

those numbers (the text) are pointing to a specific 'coordinates' in n dimensional representation. 
each point in that space is an specific representation of a token.

a embedding table is used to encode/decode.

in gpt2 the tokenizer vocabulary have ~50k tokens (~100k in gpt3-5 and 4). the contenxt size is 1024 tokens (that mean that is taking attention to 1024 tokens)

one of the main issues with tokenizer and languages were how they tokenize languages but english. any sentence in a language with diff alphabet and/or a way of write just tokenize with a lot more of tokens that english. this make bloating up all the text and it's separated across way too much of the sequence. this kind of situations change with modern models like gpt3 or gpt4.

a lot of side situations are (were) or can be (could be) related to tokenization.
- can't spell words
- bad at arithmetic
- work better with YAML than JSON
- can't do simple string processing

if we go from gpt2 to gpt4 tokenizer, we'll see that the tokenizer 'win' almost x2 of context attention because the tokenizer is incledible better. (we need to have in consideration the context window, the tokens that the model can have attention to, and amount of tokens generated by tokenizer) why? simple because gpt4 tokenizer use less tokens to represent the same amount of text.

we can see this in: 
- japanese and korean.
- code (it's INCREDIBLE better with white spaces)

as i said before, this laids to more 'compressed' representation of sentences. this means, more context can be in the attention window.

as we cannot supply the llm with raw text or bytes (chars representation as bytes) because the context lenght would useless for attention (any sentence would be too long), we gotta use:

byte pair encoding algorithm (BPE): 
- first of all we've to assume that all unique characters are initialized to a 1-char long n-gram (initial "token"). 
- then, successively the most frequent pair of adjacent characters is related to a new 2-char long n-gram, which will replace the pair of characters in any sentence with this new token generated. 
- this workflow is repeated until a vocabulary of 'x' size is created. any new work will be constructed from final representation (vocabulary of tokens) and the initial set of individual characters. 
- all the unique tokens generated from a specific corpus, ios called 'token vocabulary'

example from BPE wiki:
aaabdaaabac -> notice pair of 'aa'

ZabdZabac -> pair of 'aa' now is represented as Z
Z=aa

ZYdZYac -> pair of 'ab' is repeated, now represented as Y 
Y=ab
Z=aa

XdXac -> pair of 'ZY' is repeated, now represented as X 
X=ZY
Y=ab
Z=aa

of course we've to know that the tokenizer is completly isolated from the LLM architecture. so it have his own train loop, dataset.

basically if we've to decompose a simple tokenizer we need:
- get_pairs(tokens) -> function to obtain the repeated pairs in the dataset (text):
is an iteration between the encoded text (dataset) using utf-8 (characters representation in raw bytes, 0...255). in that iteration the pairs repeated are count and store as tuple.

- merge(tokens, idx_start, pairs) -> function to map the top pairs tuple to new id representation:
with the list of tuples done we set a convenience number to start from our 'new indexes', in this case 256. why? bc our numbers are represented in utf-8 that goes from 0 to 255.
we've the tokens (bytes representation done before even get the pairs), index to start from (256) and the pairs (top_pair will be used in the function) we wanna represent with a new index.
we just iterate through the tokens and: if an specific token matches with 'pair[0]' and the next one matches with 'pair[1]' is appended to a new ids list. the iteration is done in all the tokens, one by one. 

*of course this functions must be run in loop to work with a lot of pairs and sentence tokens.

when the vocab is done:
- encode(raw_text) -> function to transfor raw text into tokens, using the map (dict in python) created with merge():
start encoding the text in tokens with utf-8, as is done before get the pairs. why? 
it will do get all the pairs repeated again with those tokens with 'get_pairs(tokens)'.
then, filter the pairs got from merge function (each pair and new representation). 
get idx_start from merges[pair] (final pair represented), get tokens from merge() and that's it. tokens returned.

- decode(ids) -> function to transform encoded text (aka ids, in the map that we've created) to plain text/string:
'ids' would be a list of numerical representation of text (if you don't get it just skip, tokens man, tokens), basically sentence/s tokenized.
those tokens are 'mapped' with 'vocab' (vocab is constructed from bytes representation (256 chars) and mapped to 'merges', the dict of pairs representation. basically the vocab with the one we encode).
tokens, now, are just a string like {b"my name is valentin"}, which is a byte string actually. that byte string is decoded using utf-8, to finally, give us the text.



























